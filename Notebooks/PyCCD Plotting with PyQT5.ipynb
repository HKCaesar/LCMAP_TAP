{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"QT5Agg\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.collections import PathCollection\n",
    "\n",
    "from PyQt5 import QtWidgets\n",
    "\n",
    "from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_qt5agg import NavigationToolbar2QT as NavigationToolbar\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# %matplotlib qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some helper methods and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GeoExtent = namedtuple('GeoExtent', ['x_min', 'y_max', 'x_max', 'y_min'])\n",
    "GeoAffine = namedtuple('GeoAffine', ['ul_x', 'x_res', 'rot_1', 'ul_y', 'rot_2', 'y_res'])\n",
    "GeoCoordinate = namedtuple('GeoCoordinate', ['x', 'y'])\n",
    "RowColumn = namedtuple('RowColumn', ['row', 'column'])\n",
    "RowColumnExtent = namedtuple('RowColumnExtent', ['start_row', 'start_col', 'end_row', 'end_col'])\n",
    "CONUS_EXTENT = GeoExtent(x_min=-2565585,\n",
    "                         y_min=14805,\n",
    "                         x_max=2384415,\n",
    "                         y_max=3314805)\n",
    "bands = ('blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal')\n",
    "indices = ('ndvi', 'msavi', 'evi', 'savi', 'ndmi', 'nbr', 'nbr2')\n",
    "band_info = {b: {'coefs': [], 'inter': [], 'pred': []} for b in bands}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required parameters manually entered here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_DIR = r'Z:\\sites\\chesapeake\\pyccd-results\\H28V09\\2017.08.18\\json'\n",
    "\n",
    "CACHE_DIR = r'Z:\\sites\\chesapeake\\ARD\\h28v09\\cache'\n",
    "\n",
    "arc_paste = '1772368 1827300'\n",
    "\n",
    "H = 28\n",
    "\n",
    "V = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate file inventory lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_INV = [os.path.join(JSON_DIR, f) for f in os.listdir(JSON_DIR)]\n",
    "\n",
    "CACHE_INV = [os.path.join(CACHE_DIR, f) for f in os.listdir(CACHE_DIR)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set time parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These values are hardcoded for now\n",
    "# May want to switch to determining these from the PyCCD results or ARD obs.\n",
    "BEGIN_DATE = dt.date(year=1982, month=1, day=1)\n",
    "END_DATE = dt.date(year=2015, month=12, day=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geospatial_hv(h, v, loc):\n",
    "    \"\"\"\n",
    "    Geospatial extent and 30m affine for a given ARD grid location.\n",
    "    \"\"\"\n",
    "    xmin = loc.x_min + h * 5000 * 30\n",
    "    xmax = loc.x_min + h * 5000 * 30 + 5000 * 30\n",
    "    ymax = loc.y_max - v * 5000 * 30\n",
    "    ymin = loc.y_max - v * 5000 * 30 - 5000 * 30\n",
    "\n",
    "    return (GeoExtent(x_min=xmin, x_max=xmax, y_max=ymax, y_min=ymin),\n",
    "            GeoAffine(ul_x=xmin, x_res=30, rot_1=0, ul_y=ymax, rot_2=0, y_res=-30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_to_rowcol(affine, coord):\n",
    "    \"\"\"\n",
    "    Transform geo-coordinate to row/col given a reference affine.\n",
    "    \n",
    "    Yline = (Ygeo - GT(3) - Xpixel*GT(4)) / GT(5)\n",
    "    Xpixel = (Xgeo - GT(0) - Yline*GT(2)) / GT(1)\n",
    "    \"\"\"\n",
    "    row = (coord.y - affine.ul_y - affine.ul_x * affine.rot_2) / affine.y_res\n",
    "    col = (coord.x - affine.ul_x - affine.ul_y * affine.rot_1) / affine.x_res\n",
    "\n",
    "    return RowColumn(row=int(row),\n",
    "                     column=int(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rowcol_to_geo(affine, rowcol):\n",
    "    \"\"\"\n",
    "    Transform a row/col into a geospatial coordinate given reference affine.\n",
    "    \n",
    "    Xgeo = GT(0) + Xpixel*GT(1) + Yline*GT(2)\n",
    "    Ygeo = GT(3) + Xpixel*GT(4) + Yline*GT(5)\n",
    "    \"\"\"\n",
    "    x = affine.ul_x + rowcol.column * affine.x_res + rowcol.row * affine.rot_1\n",
    "    y = affine.ul_y + rowcol.column * affine.rot_2 + rowcol.row * affine.y_res\n",
    "\n",
    "    return GeoCoordinate(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cache(file):\n",
    "    \"\"\"\n",
    "    Load the cache file and split the data into the image IDs and values\n",
    "    \"\"\"\n",
    "    data = np.load(file)\n",
    "    return data['Y'], data['image_IDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_file(file_ls, string):\n",
    "    \"\"\"\n",
    "    Return the first str in a list of strings that contains string.\n",
    "    \"\"\"\n",
    "    gen = filter(lambda x: string in x, file_ls)\n",
    "    return next(gen, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imageid_date(image_ids):\n",
    "    \"\"\"\n",
    "    Extract the ordinal day from the ARD image name.\n",
    "    \"\"\"\n",
    "    return np.array([dt.datetime.strptime(d[15:23], '%Y%m%d').toordinal()\n",
    "                     for d in image_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_daterange(dates):\n",
    "    \"\"\"\n",
    "    Create a mask for values outside of the global BEGIN_DATE and END_DATE.\n",
    "    :param dates:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.logical_and(dates >= BEGIN_DATE.toordinal(), dates < END_DATE.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_chipcurve(results_chip, coord):\n",
    "    \"\"\"\n",
    "    Find the results for the specified coordinate.\n",
    "    \"\"\"\n",
    "    with open(results_chip, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    gen = filter(lambda x: coord.x == x['x'] and coord.y == x['y'], results)\n",
    "    \n",
    "    return next(gen, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cachepoint(coord):\n",
    "    \"\"\"\n",
    "    Extract the spectral values from the cache file.\n",
    "    \"\"\"\n",
    "\n",
    "    rowcol = geo_to_rowcol(PIXEL_AFFINE, coord)\n",
    "    \n",
    "    data, image_ids = load_cache(find_file(CACHE_INV, 'r{}'.format(rowcol.row)))\n",
    "    \n",
    "    dates = imageid_date(image_ids)\n",
    "    \n",
    "    return image_ids, data[:, :, rowcol.column], dates\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_jsoncurve(coord):\n",
    "        \"\"\"\n",
    "        Extract the pyccd information from the json file representing a chip of results.\n",
    "        \"\"\"\n",
    "        pixel_rowcol = geo_to_rowcol(PIXEL_AFFINE, coord)\n",
    "        pixel_coord = rowcol_to_geo(PIXEL_AFFINE, pixel_rowcol)\n",
    "\n",
    "        chip_rowcol = geo_to_rowcol(CHIP_AFFINE, coord)\n",
    "        chip_coord = rowcol_to_geo(CHIP_AFFINE, chip_rowcol)\n",
    "\n",
    "        file = find_file(JSON_INV,\n",
    "                              \"H{:02d}V{:02d}_{}_{}.json\".format(H, V, chip_coord.x, chip_coord.y))\n",
    "        result = find_chipcurve(file, pixel_coord)\n",
    "\n",
    "        return json.loads(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predicts(days, coef, intercept):\n",
    "    return (intercept + coef[0] * days +\n",
    "            coef[1]*np.cos(days*1*2*np.pi/365.25) + coef[2]*np.sin(days*1*2*np.pi/365.25) +\n",
    "            coef[3]*np.cos(days*2*2*np.pi/365.25) + coef[4]*np.sin(days*2*2*np.pi/365.25) +\n",
    "            coef[5]*np.cos(days*3*2*np.pi/365.25) + coef[6]*np.sin(days*3*2*np.pi/365.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arcpaste_to_coord(string):\n",
    "    pieces = string.split()\n",
    "    \n",
    "    return GeoCoordinate(x=float(re.sub(',', '', pieces[0])),\n",
    "                         y=float(re.sub(',', '', pieces[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data(end_date, dates_in, dates_out, data, dates, date_mask, ccd_mask):\n",
    "    \"\"\"\n",
    "    Test the dates for the presence of duplicates, and compare both with and without duplicate counts to the number\n",
    "    of observations in the PyCCD internal processing mask.  One possible source of duplicates is equal date\n",
    "    observations from Landsat 7 and 8.  During Landsat 8's ascension into orbit there was a brief period of time\n",
    "    where the two sensors 'overlapped' to allow instrument calibration, so duplicate acquisitions are possible but\n",
    "    shouldn't be present because these particular Landsat 8 observations should have been removed from the ARD\n",
    "    source directory.\n",
    "    Another potential source of duplicate observations (i.e. dates) is when the ARD is re-ingested, it's scene ID\n",
    "    may contain a different access date.  For example:\n",
    "    LE07_CU_013005_20041223_20170731_C01_V01\n",
    "    LE07_CU_013005_20041223_20170801_C01_V01\n",
    "    These folders contain the same Landsat 7 observation acquired on 2004-12-23 but they have different accessed\n",
    "    dates which is why one didn't overwrite the other.\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO return either the second or first of duplicate pairs, need to figure out which (does it matter?)\n",
    "\n",
    "    if len(dates_in) == len(ccd_mask):\n",
    "\n",
    "        print(\"The number of observations is consistent with the length of the PyCCD internal processing mask.\\n\"\n",
    "              \"No changes to the input observations are necessary.\")\n",
    "\n",
    "        return end_date, dates_in, dates_out, data, date_mask, ccd_mask\n",
    "\n",
    "    if len(np.unique(dates_in)) != len(dates_in) and len(np.unique(dates_in)) == len(ccd_mask):\n",
    "\n",
    "        print(\"There is a duplicate date occurrence in observations.  Removing duplicate occurrences makes the \"\n",
    "              \"number of observations consistent with the length of the PyCCD internal processing mask.\")\n",
    "\n",
    "        dupes = [item for item, count in Counter(dates).items() if count > 1]\n",
    "\n",
    "        dates, ind, counts = np.unique(dates, return_index=True, return_counts=True)\n",
    "\n",
    "        print(f\"Duplicate dates: \\n\\t{[dt.datetime.fromordinal(d) for d in dupes]}\")\n",
    "\n",
    "        data = data[:, ind]\n",
    "\n",
    "        date_mask = mask_daterange(dates)\n",
    "\n",
    "        dates_in = dates[date_mask]\n",
    "\n",
    "        dates_out = dates[~date_mask]\n",
    "\n",
    "        return end_date, dates_in, dates_out, data, date_mask, ccd_mask\n",
    "\n",
    "    if len(dates_in) != len(ccd_mask) and len(np.unique(dates_in)) != len(ccd_mask):\n",
    "\n",
    "        # TODO Must check the date mask range, some tiles are inclusive of the end date\n",
    "        end_date = dt.date(year=2016, month=1, day=1)\n",
    "\n",
    "        date_mask = mask_daterange(dates)\n",
    "\n",
    "        dates_in = dates[date_mask]\n",
    "        dates_out = dates[~date_mask]\n",
    "\n",
    "        # Try using the inclusive date mask\n",
    "        if len(dates_in) == len(ccd_mask):\n",
    "            print(\n",
    "                \"The number of observations is consistent with the length of the PyCCD internal processing mask.\\n\"\n",
    "                \"No changes to the input observations are necessary.\")\n",
    "\n",
    "            return end_date, dates_in, dates_out, data, date_mask, ccd_mask\n",
    "\n",
    "        # If the inclusive date mask doesn't match the processing mask, then resort to using the PIXELQA\n",
    "        else:\n",
    "            print(\"There is an inconsistency with the length of the processing mask, therefore it will not be used.\"\n",
    "                \"  PIXELQA band will be used to filter observations.\")\n",
    "            ccd_mask = get_pqa_mask()\n",
    "\n",
    "            return end_date, dates_in, dates_out, data, date_mask, ccd_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicts(num):\n",
    "    \"\"\"\n",
    "    Return the list of model prediction values in the time series for a particular band or bands\n",
    "\n",
    "    :param num: int or list\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for type int, create list if true\n",
    "    if isinstance(num, int):\n",
    "        num = [num]\n",
    "\n",
    "    return [predicted_values[m * len(bands) + n] for n in num\n",
    "            for m in range(len(results[\"change_models\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pqa_mask(qa, date_mask):\n",
    "    \"\"\"\n",
    "    Generate a mask from the Pixel QA\n",
    "    :param qa:\n",
    "    :param date_mask:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    clr_vals = [66, 68, 322, 324]\n",
    "\n",
    "    pixelqa_in = qa[date_mask]\n",
    "\n",
    "    pixelqa_mask = np.zeros_like(pixelqa_in, dtype=np.bool)\n",
    "\n",
    "    for val in clr_vals:\n",
    "        pixelqa_mask[pixelqa_in == val] = True\n",
    "\n",
    "    return pixelqa_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def msavi(R, NIR):\n",
    "    # Modified Soil Adjusted Vegetation Index\n",
    "    \n",
    "    return (2.0 * NIR + 1.0 - ((2.0 * NIR + 1.0)**2.0 - 8.0 * (NIR - R))**0.5) / 2.0\n",
    "\n",
    "def ndvi(R, NIR):\n",
    "    # Normalized Difference Vegetation Index\n",
    "    \n",
    "    return (NIR - R) / (NIR + R)\n",
    "\n",
    "def evi(B, R, NIR, G=2.5, L=1.0, C1=6, C2=7.5):\n",
    "    # Enhanced Vegetation Index\n",
    "    \n",
    "    return G * ((NIR - R) / (NIR + C1 * R - C2 * B + L))\n",
    "\n",
    "def savi(R, NIR, L=0.5):\n",
    "    # Soil Adjusted Vegetation Index\n",
    "    \n",
    "    return ((NIR - R) / (NIR + R + L)) * (1 + L)\n",
    "\n",
    "def ndmi(NIR, SWIR1):\n",
    "    # Normalized Difference Moisture Index\n",
    "    \n",
    "    return (NIR - SWIR1) / (NIR + SWIR1)\n",
    "\n",
    "def nbr(NIR, SWIR2):\n",
    "    # Normalized Burn Ratio\n",
    "    \n",
    "    return (NIR - SWIR2) / (NIR + SWIR2)\n",
    "\n",
    "def nbr2(SWIR1, SWIR2):\n",
    "    # Normalized Burn Ratio 2\n",
    "    \n",
    "    return (SWIR1 - SWIR2) / (SWIR1 + SWIR2)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce necessary geo information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coord = arcpaste_to_coord(arc_paste)\n",
    "\n",
    "EXTENT, PIXEL_AFFINE = geospatial_hv(H, V, CONUS_EXTENT)\n",
    "\n",
    "CHIP_AFFINE = GeoAffine(ul_x=PIXEL_AFFINE.ul_x, x_res=3000, rot_1=0, ul_y=PIXEL_AFFINE.ul_y, rot_2=0, y_res=-3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = extract_jsoncurve(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load observation information from the cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageIDs, data, dates = extract_cachepoint(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the date mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_mask = mask_daterange(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate an array containing the results' internal processing mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccd_mask = np.array(results[\"processing_mask\"], dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of dates in/out of the BEGIN-to-END date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_in = dates[date_mask]\n",
    "dates_out = dates[~date_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an array of the PIXELQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qa = data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the length of the observed values against the length of the processing mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations is consistent with the length of the PyCCD internal processing mask.\n",
      "No changes to the input observations are necessary.\n"
     ]
    }
   ],
   "source": [
    "END_DATE, dates_in, dates_out, data, date_mask, ccd_mask = test_data(end_date=END_DATE, dates_in=dates_in, \n",
    "                                                                     dates_out=dates_out, data=data, dates=dates, \n",
    "                                                                     date_mask=date_mask, ccd_mask=ccd_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate mask based on the PIXELQA fill value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fill_mask = np.ones_like(qa, dtype=np.bool)\n",
    "fill_mask[qa == 1] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the PIXELQA masks in/out of the BEGIN-END date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fill_in = fill_mask[date_mask]\n",
    "fill_out = fill_mask[~date_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a total mask by combining the internal mask and the PIXELQA-in-date-range mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_mask = np.logical_and(ccd_mask, fill_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the brightness temperature to match the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_thermal_data = np.copy(data[6])\n",
    "temp_thermal_data[fill_mask] = temp_thermal_data[fill_mask] * 10 - 27315\n",
    "data[6] = np.copy(temp_thermal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create containers that will store lists of values for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_values = []\n",
    "prediction_dates = []\n",
    "break_dates = []\n",
    "start_dates = []\n",
    "end_dates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and display information from the PyCCD results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0\n",
      "Start Date: 1982-11-11\n",
      "End Date: 2014-11-03\n",
      "Break Date: 2014-11-03\n",
      "QA: 8\n",
      "Change prob: 0\n"
     ]
    }
   ],
   "source": [
    "for num, result in enumerate(results['change_models']):\n",
    "    print('Result: {}'.format(num))\n",
    "    print('Start Date: {}'.format(dt.date.fromordinal(result['start_day'])))\n",
    "    print('End Date: {}'.format(dt.date.fromordinal(result['end_day'])))\n",
    "    print('Break Date: {}'.format(dt.date.fromordinal(result['break_day'])))\n",
    "    print('QA: {}'.format(result['curve_qa']))\n",
    "    print('Change prob: {}'.format(result['change_probability']))\n",
    "    \n",
    "    days = np.arange(result['start_day'], result['end_day'] + 1)\n",
    "    \n",
    "    break_dates.append(result['break_day'])\n",
    "    start_dates.append(result['start_day'])\n",
    "    end_dates.append(result['end_day'])\n",
    "    \n",
    "    for b in bands:\n",
    "        band_info[b]['inter'] = result[b]['intercept']\n",
    "        band_info[b]['coefs'] = result[b]['coefficients']\n",
    "        \n",
    "        band_info[b]['pred'] = predicts(days, result[b]['coefficients'], result[b]['intercept'])\n",
    "        \n",
    "        prediction_dates.append(days)\n",
    "        predicted_values.append(band_info[b]['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate indices from observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dzelenak\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "EVI = evi(B=data[0], NIR=data[3], R=data[2])\n",
    "\n",
    "NDVI = ndvi(R=data[2], NIR=data[3])\n",
    "\n",
    "MSAVI = msavi(R=data[2], NIR=data[3])\n",
    "\n",
    "SAVI = savi(R=data[2], NIR=data[3])\n",
    "\n",
    "NDMI = ndmi(NIR=data[3], SWIR1=data[4])\n",
    "\n",
    "NBR = nbr(NIR=data[3], SWIR2=data[5])\n",
    "\n",
    "NBR2 = nbr2(SWIR1=data[4], SWIR2=data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate indices from the results' change models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The change models are stored by order of model, then band number.  For example, the band values for the first change \n",
    "# model are represented by indices 0-5, the second model by indices 6-11, and so on.\n",
    "\n",
    "NDVI_ = [ndvi(NIR=predicted_values[m * len(bands) + 3],\n",
    "                                  R=predicted_values[m * len(bands) + 2])\n",
    "              for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "MSAVI_ = [msavi(R=predicted_values[m * len(bands) + 2],\n",
    "                                    NIR=predicted_values[m * len(bands) + 3])\n",
    "               for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "EVI_ = [evi(B=predicted_values[m * len(bands)],\n",
    "                                NIR=predicted_values[m * len(bands) + 3],\n",
    "                                R=predicted_values[m * len(bands) + 2])\n",
    "             for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "SAVI_ = [savi(NIR=predicted_values[m * len(bands) + 3],\n",
    "                                  R=predicted_values[m * len(bands) + 2])\n",
    "              for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "NDMI_ = [ndmi(NIR=predicted_values[m * len(bands) + 3],\n",
    "                                  SWIR1=predicted_values[m * len(bands) + 4])\n",
    "              for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "NBR_ = [nbr(NIR=predicted_values[m * len(bands) + 3],\n",
    "                                SWIR2=predicted_values[m * len(bands) + 5])\n",
    "             for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "NBR2_ = [nbr2(SWIR1=predicted_values[m * len(bands) + 4],\n",
    "                                  SWIR2=predicted_values[m * len(bands) + 5])\n",
    "              for m in range(len(results[\"change_models\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_lookup = {\"NDVI\": (NDVI, NDVI_),\n",
    "                 \"MSAVI\": (MSAVI, MSAVI_),\n",
    "                 \"EVI\": (EVI, EVI_),\n",
    "                 \"SAVI\": (SAVI, SAVI_),\n",
    "                 \"NDMI\": (NDMI, NDMI_),\n",
    "                 \"NBR\": (NBR, NBR_),\n",
    "                 \"NBR-2\": (NBR2, NBR2_)}\n",
    "\n",
    "band_lookup = {\"Blue\": (data[0], get_predicts(0)),\n",
    "                \"Green\": (data[1], get_predicts(1)),\n",
    "                \"Red\": (data[2], get_predicts(2)),\n",
    "                \"NIR\": (data[3], get_predicts(3)),\n",
    "                \"SWIR-1\": (data[4], get_predicts(4)),\n",
    "                \"SWIR-2\": (data[5], get_predicts(5)),\n",
    "                \"Thermal\": (data[6], get_predicts(6))}\n",
    "\n",
    "# Combine these two dictionaries\n",
    "all_lookup = {**band_lookup, **index_lookup}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which dictionary to use for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_data = all_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some plot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "# get year values for labeling plots\n",
    "year1 = str(dt.datetime.fromordinal(dates[0]))[:4]\n",
    "year2 = str(dt.datetime.fromordinal(dates[-1]))[:4]\n",
    "\n",
    "# List every-other year\n",
    "years = range(int(year1), int(year2) + 2, 2)\n",
    "\n",
    "# List every-single year\n",
    "years_ = range(int(year1), int(year2) + 2, 1)\n",
    "\n",
    "# list of datetime objects with YYYY-MM-dd pattern using July 1 for month and day\n",
    "t = [dt.datetime(yx, 7, 1) for yx in years]\n",
    "\n",
    "# list of datetime objects with YYYY-MM-dd pattern using January 1 for month and day\n",
    "t_ = [dt.datetime(yx, 1, 1) for yx in years_]\n",
    "\n",
    "# list of ordinal time objects\n",
    "ord_time = [dt.datetime.toordinal(tx) for tx in t]\n",
    "\n",
    "# list of datetime formatted strings\n",
    "x_labels = [str(dt.datetime.fromordinal(int(L)))[:10] if L != \"0.0\" and L != \"\" else \"0\" for L in ord_time]\n",
    "\n",
    "# decide whether or not to draw masked values\n",
    "masked_on = True\n",
    "\n",
    "# decide whether or not to draw model values\n",
    "model_on = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify inidividual bands here if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Blue', 'Green', 'Red', 'NIR', 'SWIR-1', 'SWIR-2', 'Thermal', 'NDVI', 'MSAVI', 'EVI', 'SAVI', 'NDMI', 'NBR', 'NBR-2'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys = plot_data.keys()\n",
    "all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = [\"Blue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for clicking on the plot to select the nearest artist with a set picker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://matplotlib.org/examples/event_handling/pick_event_demo.html\n",
    "def line_picker(line, mouseevent):\n",
    "    \"\"\"\n",
    "    find the points within a certain distance from the mouseclick in\n",
    "    data coords and attach some extra attributes, pickx and picky\n",
    "    which are the data points that were picked\n",
    "    \"\"\"\n",
    "    if mouseevent.xdata is None:\n",
    "        return False, dict()\n",
    "    \n",
    "    xdata = line.get_xdata()\n",
    "    ydata = line.get_ydata()\n",
    "    \n",
    "    maxd = 1.0\n",
    "    d = np.sqrt((xdata - mouseevent.xdata)**2. + (ydata - mouseevent.ydata)**2.)\n",
    "\n",
    "    ind = np.nonzero(np.less_equal(d, maxd))\n",
    "    \n",
    "    if len(ind):\n",
    "        pickx = np.take(xdata, ind)\n",
    "        picky = np.take(ydata, ind)\n",
    "        props = dict(ind=ind, pickx=pickx, picky=picky)\n",
    "        return True, props\n",
    "    \n",
    "    else:\n",
    "        return False, dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a (currently empty) mapping of data series to artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render the artist primitives and containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the artist containers\n",
    "fig, axes = plt.subplots(nrows=len(keys), ncols=1, figsize=(18, len(keys) * 6), dpi=65, squeeze=False)\n",
    "\n",
    "for num, b in enumerate(keys):\n",
    "    # Plot the observations within the PyCCD time range\n",
    "    points1 = axes[num, 0].scatter(x=dates_in[total_mask], y=plot_data[b][0][date_mask][total_mask], s=44, c=\"green\", marker='o', edgecolors=\"k\",\n",
    "                         label=\"Obs.\", picker=5)\n",
    "    \n",
    "    artist_map[points1] = [dates_in[total_mask], plot_data[b][0][date_mask][total_mask]]\n",
    "    \n",
    "    # Plot the observations outside of the PyCCD time range\n",
    "    points2 = axes[num, 0].scatter(x=dates_out[fill_out], y=plot_data[b][0][~date_mask][fill_out], s=21, c='red', marker='o', edgecolors='k', \n",
    "                      label=\"Unused\", picker=5)\n",
    "    \n",
    "    artist_map[points2] = [dates_out[fill_out], plot_data[b][0][~date_mask][fill_out]]\n",
    "    \n",
    "    # Plot the observed values masked out by PyCCD\n",
    "    if masked_on is True:\n",
    "        \n",
    "        #Remove the 0-valued masked obserations for the index plots\n",
    "        if b in keys:\n",
    "            index_plot = plot_data[b][0][date_mask][~ccd_mask]\n",
    "            \n",
    "            points3 = axes[num, 0].scatter(x=dates_in[~ccd_mask][index_plot != 0], y=index_plot[index_plot != 0], s=21, c=\"0.65\",\n",
    "                             marker=\"o\", label=\"Masked\", picker=5)\n",
    "            \n",
    "            artist_map[points3] = [dates_in[~ccd_mask][index_plot != 0], index_plot[index_plot != 0]]\n",
    "            \n",
    "        else:\n",
    "            points3 = axes[num, 0].scatter(x=dates_in[~ccd_mask], y=plot_data[b][0][date_mask][~ccd_mask], s=21, c=\"0.65\", \n",
    "                             marker=\"o\", label=\"Masked\", picker=5)\n",
    "            \n",
    "            artist_map[points3] = [dates_in[~ccd_mask], plot_data[b][0][date_mask][~ccd_mask]]\n",
    "            \n",
    "    # Give each subplot a title\n",
    "    axes[num, 0].set_title(f\"{b}\")\n",
    "    \n",
    "    # Plot the model start, end, and break days\n",
    "    if model_on is True:\n",
    "        # Get list of instances where break=start date\n",
    "        match_dates = [b for b in break_dates for s in start_dates if b==s]\n",
    "    \n",
    "        for ind, e in enumerate(end_dates):\n",
    "            if ind == 0:\n",
    "                axes[num, 0].axvline(e, color=\"maroon\", linewidth=1.5, label=\"End\", picker=line_picker)\n",
    "                \n",
    "            else:\n",
    "                # Plot without a label to remove duplicates in the legend\n",
    "                axes[num, 0].axvline(e, color=\"maroon\", linewidth=1.5, picker=None)\n",
    "                \n",
    "        for ind, br in enumerate(break_dates):\n",
    "            if ind == 0:\n",
    "                axes[num, 0].axvline(br, color=\"r\", linewidth=1.5, label=\"Break\", picker=line_picker)\n",
    "                \n",
    "            else:\n",
    "                axes[num, 0].axvline(br, color=\"r\", linewidth=1.5, picker=line_picker)\n",
    "                \n",
    "        for ind, s in enumerate(start_dates):\n",
    "            if ind == 0:\n",
    "                axes[num, 0].axvline(s, color=\"b\", linewidth=1.5, label=\"Start\", picker=line_picker)\n",
    "                \n",
    "            else:\n",
    "                axes[num, 0].axvline(s, color=\"b\", linewidth=1.5, picker=line_picker)\n",
    "                \n",
    "        for ind, m in enumerate(match_dates):\n",
    "            if ind == 0:\n",
    "                axes[num, 0].axvline(m, color=\"magenta\", linewidth=1, label=\"Break=Start\", picker=line_picker)\n",
    "                \n",
    "            else:\n",
    "                axes[num, 0].axvline(m, color=\"magenta\", linewidth=1, picker=line_picker)\n",
    "                \n",
    "        # Predicted Curves\n",
    "        for c in range(0, len(results[\"change_models\"])):\n",
    "            if c == 0:\n",
    "                axes[num, 0].plot(prediction_dates[c * len(bands)], plot_data[b][1][c], \"orange\",\n",
    "                                 linewidth=2, alpha=0.8, label=\"Model Fit\", picker=line_picker)\n",
    "                \n",
    "            else:\n",
    "                axes[num, 0].plot(prediction_dates[c * len(bands)], plot_data[b][1][c], \"orange\",\n",
    "                                 linewidth=2, alpha=0.8, picker=line_picker)\n",
    "                \n",
    "    # Get ymin and ymax values to constrain the plot window size\n",
    "    if b in index_lookup.keys():\n",
    "        ymin = min(plot_data[b][0][date_mask][total_mask]) - 0.15\n",
    "        ymax = max(plot_data[b][0][date_mask][total_mask]) + 0.1\n",
    "\n",
    "    else:\n",
    "        ymin = min(plot_data[b][0][date_mask][total_mask]) - 700\n",
    "        ymax = max(plot_data[b][0][date_mask][total_mask]) + 500\n",
    "\n",
    "    axes[num, 0].set_ylim([ymin, ymax])\n",
    "\n",
    "    # Add x-ticks and x-tick_labels\n",
    "    # axes[num, 0].set_xticks(ord_time)\n",
    "\n",
    "    # axes[num, 0].set_xticklabels(x_labels, rotation=70, horizontalalignment=\"right\")\n",
    "\n",
    "    # Display the x and y values where the cursor is placed on a subplot\n",
    "    axes[num, 0].format_coord = lambda x, y: \"({0:f}, \".format(y) +  \\\n",
    "                                             \"{0:%Y-%m-%d})\".format(dt.datetime.fromordinal(int(x)))\n",
    "\n",
    "    # Plot a vertical line at January 1 of each year on the time series\n",
    "    for y in t_:\n",
    "        axes[num, 0].axvline(y, color=\"dimgray\", linewidth=1.5, picker=None)\n",
    "\n",
    "    # Only add a legend to the first subplot to avoid repetition and wasted space\n",
    "    if b == list(keys)[0]:\n",
    "        axes[num, 0].legend(ncol=1, loc=\"upper left\", bbox_to_anchor=(1.0, 1.0),\n",
    "                            borderaxespad=0.)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MplCanvas(FigureCanvas):\n",
    "    def __init__(self, fig):\n",
    "        self.fig = fig\n",
    "        \n",
    "        FigureCanvas.__init__(self, self.fig)\n",
    "        \n",
    "        if len(fig.axes) >= 3:\n",
    "            sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Ignored, QtWidgets.QSizePolicy.Minimum)\n",
    "        \n",
    "        else:\n",
    "            sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Ignored, QtWidgets.QSizePolicy.Ignored)\n",
    "        \n",
    "        sizePolicy.setHorizontalStretch(1)\n",
    "        sizePolicy.setVerticalStretch(1)\n",
    "        \n",
    "        FigureCanvas.setSizePolicy(self, sizePolicy)\n",
    "        \n",
    "        FigureCanvas.updateGeometry(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42622146/scrollbar-on-matplotlib-showing-page\n",
    "class PlotWindow(QtWidgets.QMainWindow):\n",
    "    def __init__(self, fig):\n",
    "        self.qapp = QtWidgets.QApplication([])\n",
    "        \n",
    "        QtWidgets.QMainWindow.__init__(self)\n",
    "        \n",
    "        self.widget = QtWidgets.QWidget()\n",
    "        self.setCentralWidget(self.widget)\n",
    "        self.widget.setLayout(QtWidgets.QVBoxLayout())\n",
    "        self.widget.layout().setContentsMargins(0,0,0,0)\n",
    "        self.widget.layout().setSpacing(0)\n",
    "        \n",
    "        self.fig = fig\n",
    "        self.canvas = MplCanvas(self.fig)\n",
    "        self.canvas.draw()\n",
    "            \n",
    "        \n",
    "        # def line_pick(event):\n",
    "            # event.pickx = the x-data value at the nearset point\n",
    "            # event.picky = the y-data value at the nearest point\n",
    "            # print('onpick line:', dt.datetime.fromordinal(int(event.pickx)), event.picky)\n",
    "            \n",
    "        def point_pick(event):\n",
    "            \n",
    "            mouseevent = event.mouseevent\n",
    "            \n",
    "            artist = event.artist\n",
    "            \n",
    "            # Only works using left-click (event.mouseevent.button==1) \n",
    "            # and on any of the scatter point series (PathCollection artists)\n",
    "            if isinstance(artist, PathCollection) and mouseevent.button==1:\n",
    "            \n",
    "                ind = event.ind\n",
    "                # ax = event.inaxes\n",
    "                # ax = fig.gca()\n",
    "                x=artist_map[artist][0]\n",
    "                y=artist_map[artist][1]\n",
    "                print(f'clicked button: {mouseevent.button} \\n {dt.datetime.fromordinal(int(mouseevent.xdata)), mouseevent.ydata}\\n \\\n",
    "                      ind={ind}\\n artist={artist}\\n date(x)={dt.datetime.fromordinal(int(np.take(x, ind)))}\\n \\\n",
    "                      val(y)={np.take(y, ind)}')\n",
    "            \n",
    "        \n",
    "        self.nav = NavigationToolbar(self.canvas, self.widget)\n",
    "        \n",
    "        self.widget.layout().addWidget(self.nav)\n",
    "        \n",
    "        self.widget.layout().addWidget(self.canvas)\n",
    "        \n",
    "        self.scroll = QtWidgets.QScrollArea(self.widget)\n",
    "        self.scroll.setWidgetResizable(True)\n",
    "        \n",
    "        self.scroll.setWidget(self.canvas)\n",
    "               \n",
    "        \n",
    "        self.widget.layout().addWidget(self.scroll)\n",
    "        \n",
    "        self.canvas.mpl_connect(\"pick_event\", point_pick)\n",
    "        \n",
    "        # self.canvas.mpl_connect(\"pick_event\", line_pick)\n",
    "                        \n",
    "        self.show()\n",
    "        exit(self.qapp.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked button: 1 \n",
      " (datetime.datetime(2017, 3, 17, 0, 0), 3865.717554958971)\n",
      "                       ind=[47]\n",
      " artist=<matplotlib.collections.PathCollection object at 0x0000016B88974E80>\n",
      " date(x)=2017-03-08 00:00:00\n",
      "                       val(y)=[3859]\n",
      "clicked button: 1 \n",
      " (datetime.datetime(2017, 3, 17, 0, 0), 3865.717554958971)\n",
      "                       ind=[47]\n",
      " artist=<matplotlib.collections.PathCollection object at 0x0000016B88974E80>\n",
      " date(x)=2017-03-08 00:00:00\n",
      "                       val(y)=[3859]\n",
      "clicked button: 1 \n",
      " (datetime.datetime(2017, 3, 17, 0, 0), 3865.717554958971)\n",
      "                       ind=[47]\n",
      " artist=<matplotlib.collections.PathCollection object at 0x0000016B88974E80>\n",
      " date(x)=2017-03-08 00:00:00\n",
      "                       val(y)=[3859]\n",
      "clicked button: 1 \n",
      " (datetime.datetime(2016, 7, 7, 0, 0), 7754.7993591855811)\n",
      "                       ind=[21]\n",
      " artist=<matplotlib.collections.PathCollection object at 0x0000016B88974E80>\n",
      " date(x)=2016-07-27 00:00:00\n",
      "                       val(y)=[7728]\n"
     ]
    }
   ],
   "source": [
    "# Display the figure\n",
    "look = PlotWindow(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some Notes:\n",
    "\n",
    "It is desired to be able to select any series on the plot and have returned the x-coordinate (i.e. dt.datetime.fromordinal()) and the y-coordinate (reflectance or indices value).  Need to implement something in the code that allows to differentiate between the series because currently the reference is being made to a single date series range, and the return on clicking is not correct for any items in the series after the first index. - Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to be able to draw a rectangle around groups of points and return their values as a .txt or .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
