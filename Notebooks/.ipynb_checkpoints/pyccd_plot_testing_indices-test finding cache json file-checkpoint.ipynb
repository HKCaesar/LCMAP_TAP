{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper methods and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GeoExtent = namedtuple('GeoExtent', ['x_min', 'y_max', 'x_max', 'y_min'])\n",
    "GeoAffine = namedtuple('GeoAffine', ['ul_x', 'x_res', 'rot_1', 'ul_y', 'rot_2', 'y_res'])\n",
    "GeoCoordinate = namedtuple('GeoCoordinate', ['x', 'y'])\n",
    "RowColumn = namedtuple('RowColumn', ['row', 'column'])\n",
    "RowColumnExtent = namedtuple('RowColumnExtent', ['start_row', 'start_col', 'end_row', 'end_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geospatial_hv(h, v, loc):\n",
    "    \"\"\"\n",
    "    Geospatial extent and 30m affine for a given ARD grid location.\n",
    "    \"\"\"\n",
    "    xmin = loc.x_min + h * 5000 * 30\n",
    "    xmax = loc.x_min + h * 5000 * 30 + 5000 * 30\n",
    "    ymax = loc.y_max - v * 5000 * 30\n",
    "    ymin = loc.y_max - v * 5000 * 30 - 5000 * 30\n",
    "\n",
    "    return (GeoExtent(x_min=xmin, x_max=xmax, y_max=ymax, y_min=ymin),\n",
    "            GeoAffine(ul_x=xmin, x_res=30, rot_1=0, ul_y=ymax, rot_2=0, y_res=-30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_to_rowcol(affine, coord):\n",
    "    \"\"\"\n",
    "    Transform geo-coordinate to row/col given a reference affine.\n",
    "    \n",
    "    Yline = (Ygeo - GT(3) - Xpixel*GT(4)) / GT(5)\n",
    "    Xpixel = (Xgeo - GT(0) - Yline*GT(2)) / GT(1)\n",
    "    \"\"\"\n",
    "    row = (coord.y - affine.ul_y - affine.ul_x * affine.rot_2) / affine.y_res\n",
    "    col = (coord.x - affine.ul_x - affine.ul_y * affine.rot_1) / affine.x_res\n",
    "\n",
    "    return RowColumn(row=int(row),\n",
    "                     column=int(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rowcol_to_geo(affine, rowcol):\n",
    "    \"\"\"\n",
    "    Transform a row/col into a geospatial coordinate given reference affine.\n",
    "    \n",
    "    Xgeo = GT(0) + Xpixel*GT(1) + Yline*GT(2)\n",
    "    Ygeo = GT(3) + Xpixel*GT(4) + Yline*GT(5)\n",
    "    \"\"\"\n",
    "    x = affine.ul_x + rowcol.column * affine.x_res + rowcol.row * affine.rot_1\n",
    "    y = affine.ul_y + rowcol.column * affine.rot_2 + rowcol.row * affine.y_res\n",
    "\n",
    "    return GeoCoordinate(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cache(file):\n",
    "    \"\"\"\n",
    "    Load the cache file and split the data into the image IDs and values\n",
    "    \"\"\"\n",
    "    data = np.load(file)\n",
    "    return data['Y'], data['image_IDs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_file(file_ls, string):\n",
    "    \"\"\"\n",
    "    Return the first str in a list of strings that contains string.\n",
    "    \"\"\"\n",
    "    gen = filter(lambda x: string in x, file_ls)\n",
    "    return next(gen, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imageid_date(image_ids):\n",
    "    \"\"\"\n",
    "    Extract the ordinal day from the ARD image name.\n",
    "    \"\"\"\n",
    "    return np.array([dt.datetime.strptime(d[15:23], '%Y%m%d').toordinal()\n",
    "                     for d in image_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mask_daterange(dates):\n",
    "    \"\"\"\n",
    "    Create a mask for values outside of the global BEGIN_DATE and END_DATE.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.logical_and(dates >= BEGIN_DATE.toordinal(), dates <= END_DATE.toordinal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_chipcurve(results_chip, coord):\n",
    "    \"\"\"\n",
    "    Find the results for the specified coordinate.\n",
    "    \"\"\"\n",
    "    with open(results_chip, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    gen = filter(lambda x: coord.x == x['x'] and coord.y == x['y'], results)\n",
    "    \n",
    "    return next(gen, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cachepoint(coord):\n",
    "    \"\"\"\n",
    "    Extract the spectral values from the cache file.\n",
    "    \"\"\"\n",
    "\n",
    "    rowcol = geo_to_rowcol(PIXEL_AFFINE, coord)\n",
    "    \n",
    "    data, image_ids = load_cache(find_file(CACHE_INV, 'r{}'.format(rowcol.row)))\n",
    "    \n",
    "    dates = imageid_date(image_ids)\n",
    "           \n",
    "    return image_ids, data[:, :, rowcol.column], dates\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_jsoncurve(coord):\n",
    "    \"\"\"\n",
    "    Extract the pyccd information from the json file representing a chip of results.\n",
    "    \"\"\"\n",
    "    pixel_rowcol = geo_to_rowcol(PIXEL_AFFINE, coord)\n",
    "    pixel_coord = rowcol_to_geo(PIXEL_AFFINE, pixel_rowcol)\n",
    "    \n",
    "    chip_rowcol = geo_to_rowcol(CHIP_AFFINE, coord)\n",
    "    chip_coord = rowcol_to_geo(CHIP_AFFINE, chip_rowcol)\n",
    "    \n",
    "    file = find_file(JSON_INV, 'H{:02d}V{:02d}_{}_{}.json'.format(H, V, chip_coord.x, chip_coord.y))\n",
    "    result = find_chipcurve(file, pixel_coord)\n",
    "    \n",
    "    if result.get('result_ok') is True:\n",
    "        return json.loads(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predicts(days, coef, intercept):\n",
    "    return (intercept + coef[0] * days +\n",
    "            coef[1]*np.cos(days*1*2*np.pi/365.25) + coef[2]*np.sin(days*1*2*np.pi/365.25) +\n",
    "            coef[3]*np.cos(days*2*2*np.pi/365.25) + coef[4]*np.sin(days*2*2*np.pi/365.25) +\n",
    "            coef[5]*np.cos(days*3*2*np.pi/365.25) + coef[6]*np.sin(days*3*2*np.pi/365.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arcpaste_to_coord(string):\n",
    "    pieces = string.split()\n",
    "    \n",
    "    return GeoCoordinate(x=float(re.sub(',', '', pieces[0])),\n",
    "                         y=float(re.sub(',', '', pieces[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    \"\"\"\n",
    "    Test the dates for the presence of duplicates, and compare both with and without duplicate counts to the number\n",
    "    of observations in the PyCCD internal processing mask.  One possible source of duplicates is equal date\n",
    "    observations from Landsat 7 and 8.  During Landsat 8's ascension into orbit there was a brief period of time\n",
    "    where the two sensors 'overlapped', so duplicate acquisitions are possible but shouldn't be present because\n",
    "    these particular Landsat 8 observations should have been removed from the ARD source directory.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TODO return either the second or first of duplicate pairs, need to figure out which (does it matter?)\n",
    "\n",
    "    if len(dates_in) == len(results[\"processing_mask\"]):\n",
    "        print(\"The number of observations is consistent with the length of the PyCCD internal processing mask.\\n\"\n",
    "              \"No changes to the input observations are necessary.\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    if len(np.unique(dates_in)) == len(results[\"processing_mask\"]):\n",
    "        print(\"There is a duplicate date occurrence in observations.  Removing duplicate occurrences makes the \"\n",
    "              \"number of observations consistent with the length of the PyCCD internal processing mask.\")\n",
    "\n",
    "        dupes = [item for item, count in Counter(dates.items() if count > 1)]\n",
    "        \n",
    "        dates, ind, counts = np.unique(dates, return_index=True, return_counts=True)\n",
    "\n",
    "        print(\"Duplicate dates: \\n\\t\", dates[:, ind])\n",
    "\n",
    "        data = data[:, ind]\n",
    "\n",
    "        date_mask = mask_daterange(dates)\n",
    "\n",
    "        dates_in = dates[date_mask]\n",
    "\n",
    "        dates_out = dates[~date_mask]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JSON_DIR = r'Z:\\sites\\chesapeake\\pyccd-results\\H27V09\\2017.08.18\\json'\n",
    "JSON_INV = [os.path.join(JSON_DIR, f) for f in os.listdir(JSON_DIR)]\n",
    "CACHE_DIR = r'Z:\\sites\\chesapeake\\ARD\\h27v09'\n",
    "CACHE_INV = [os.path.join(CACHE_DIR, f) for f in os.listdir(CACHE_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arc_paste = '1589066 940031'\n",
    "coord = arcpaste_to_coord(arc_paste)\n",
    "\n",
    "CONUS_EXTENT = GeoExtent(x_min=-2565585,\n",
    "                         y_min=14805,\n",
    "                         x_max=2384415,\n",
    "                         y_max=3314805)\n",
    "\n",
    "H = 27\n",
    "V = 9\n",
    "EXTENT, PIXEL_AFFINE = geospatial_hv(H, V, CONUS_EXTENT)\n",
    "CHIP_AFFINE = GeoAffine(ul_x=PIXEL_AFFINE.ul_x, x_res=3000, rot_1=0, ul_y=PIXEL_AFFINE.ul_y, rot_2=0, y_res=-3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"entered coord= \", coord)\n",
    "pixel_rowcol = geo_to_rowcol(PIXEL_AFFINE, coord)\n",
    "pixel_coord = rowcol_to_geo(PIXEL_AFFINE, pixel_rowcol)\n",
    "\n",
    "chip_rowcol = geo_to_rowcol(CHIP_AFFINE, coord)\n",
    "chip_coord = rowcol_to_geo(CHIP_AFFINE, chip_rowcol)\n",
    "\n",
    "# pixel_rowcol is used to find the cache file.  There is one cache file per row of the tile.\n",
    "print(\"pixel_rowcol= \", pixel_rowcol)\n",
    "\n",
    "print(\"pixel_coord= \", pixel_coord)\n",
    "print(\"chip_rowcol= \", chip_rowcol)\n",
    "\n",
    "# chip_coord is used to find the json file.  There are 2500 chips and json files per tile.  \n",
    "# The chip_coord gives the upper left coordinate which is used to identify the chip and corresponding json file.\n",
    "print(\"chip_coord= \", chip_coord)\n",
    "print(\"EXTENT= \", EXTENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = extract_jsoncurve(coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageIDs, data, dates = extract_cachepoint(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN_DATE = dt.datetime.fromordinal(results[\"change_models\"][0][\"start_day\"])\n",
    "BEGIN_DATE = dt.datetime.fromordinal(dates[0])\n",
    "# END_DATE = dt.datetime.fromordinal(results[\"change_models\"][-1][\"break_day\"])\n",
    "END_DATE = dt.datetime.fromordinal(dates[len(results[\"processing_mask\"])-1])\n",
    "\n",
    "print(BEGIN_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_mask = mask_daterange(dates=dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mask based on the ARD QA band to remove fill (value 1)\n",
    "qa = data[-1]\n",
    "qa_mask = np.ones_like(qa, dtype=np.bool)\n",
    "qa_mask[qa == 1] = False\n",
    "qa_in = qa_mask[date_mask]\n",
    "qa_out = qa_mask[~date_mask]\n",
    "\n",
    "print(len(qa_mask))\n",
    "print(len(qa_in))\n",
    "print(len(qa_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup geospatial and temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_in = dates[date_mask]\n",
    "dates_out = dates[~date_mask]\n",
    "\n",
    "print(len(dates_in))\n",
    "print(len(dates_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(dates))\n",
    "print(np.shape(np.unique(dates)))\n",
    "print(len(results[\"processing_mask\"]))\n",
    "print(len(dates_in))\n",
    "print(len(np.unique(dates_in)))\n",
    "\n",
    "no_fill_dates_in = dates_in[qa_in]\n",
    "print(len(no_fill_dates_in))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dates_in) == len(results[\"processing_mask\"]):\n",
    "    print(\"The number of observations is consistent with the length of the PyCCD internal processing mask.\\n\"\n",
    "          \"No changes to the input observations are necessary.\")\n",
    "\n",
    "if len(np.unique(dates_in)) == len(results[\"processing_mask\"]):\n",
    "    print(\"There is a duplicate date occurrence in observations.  Removing duplicate occurrences makes the \"\n",
    "          \"number of observations consistent with the length of the PyCCD internal processing mask.\")\n",
    "\n",
    "    dates, ind = np.unique(dates, return_index=True)\n",
    "\n",
    "    print(\"Duplicate dates: \\n\\t\", dates[ind])\n",
    "\n",
    "    data = data[:, ind]\n",
    "\n",
    "    date_mask = mask_daterange(dates)\n",
    "\n",
    "    dates_in = dates[date_mask]\n",
    "\n",
    "    dates_out = dates[~date_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_uniq, ind = np.unique(dates, return_index=True)\n",
    "print(np.shape(dates))\n",
    "print(np.shape(dates_uniq))\n",
    "print(np.shape(ind))\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_uniq2, ind2, counts = np.unique(dates, return_index=True, return_counts=True)\n",
    "\n",
    "print(counts)\n",
    "np.amax(counts)\n",
    "\n",
    "print(dt.datetime.fromordinal(dates_uniq[counts==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale the brightness temperature to match the predicted values\n",
    "temp_thermal_data = np.copy(data[6])\n",
    "# temp_thermal_data[ temp_thermal_data != -9999 ] = temp_thermal_data[ temp_thermal_data != -9999 ] * 10 - 27315\n",
    "temp_thermal_data[qa_mask] = temp_thermal_data[qa_mask] * 10 - 27315\n",
    "data[6] = np.copy(temp_thermal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = data[:, date_mask]\n",
    "data_out = data[:, ~date_mask]\n",
    "\n",
    "print(np.shape(data_in))\n",
    "print(np.shape(data_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def msavi(R, NIR):\n",
    "    # Modified Soil Adjusted Vegetation Index\n",
    "    \n",
    "    return (2.0 * NIR + 1.0 - ((2.0 * NIR + 1.0)**2.0 - 8.0 * (NIR - R))**0.5) / 2.0\n",
    "\n",
    "def ndvi(R, NIR):\n",
    "    # Normalized Difference Vegetation Index\n",
    "    \n",
    "    return (NIR - R) / (NIR + R)\n",
    "\n",
    "def evi(B, R, NIR, G=2.5, L=1.0, C1=6, C2=7.5):\n",
    "    # Enhanced Vegetation Index\n",
    "    \n",
    "    return G * ((NIR - R) / (NIR + C1 * R - C2 * B + L))\n",
    "\n",
    "def savi(R, NIR, L=0.5):\n",
    "    # Soil Adjusted Vegetation Index\n",
    "    \n",
    "    return ((NIR - R) / (NIR + R + L)) * (1 + L)\n",
    "\n",
    "def ndmi(NIR, SWIR1):\n",
    "    # Normalized Difference Moisture Index\n",
    "    \n",
    "    return (NIR - SWIR1) / (NIR + SWIR1)\n",
    "\n",
    "def nbr(NIR, SWIR2):\n",
    "    # Normalized Burn Ratio\n",
    "    \n",
    "    return (NIR - SWIR2) / (NIR + SWIR2)\n",
    "\n",
    "def nbr2(SWIR1, SWIR2):\n",
    "    # Normalized Burn Ratio 2\n",
    "    \n",
    "    return (SWIR1 - SWIR2) / (SWIR1 + SWIR2)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bands = ('blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal')\n",
    "band_info = {b: {'coefs': [], 'inter': [], 'pred': []} for b in bands}\n",
    "\n",
    "mask = np.array(results['processing_mask'], dtype=bool)\n",
    "\n",
    "\"\"\"\n",
    "print('Start Date: {0}\\nEnd Date: {1}\\n'.format(dt.datetime.fromordinal(dates[0]),\n",
    "                                                dt.datetime.fromordinal(dates[-1])))\n",
    "\"\"\"\n",
    "\n",
    "predicted_values = []\n",
    "prediction_dates = []\n",
    "break_dates = []\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "\n",
    "# get year values for labeling plots\n",
    "year1 = str(dt.datetime.fromordinal(dates[0]))[:4]\n",
    "year2 = str(dt.datetime.fromordinal(dates[-1]))[:4]\n",
    "years = np.arange(int(year1), int(year2), 2)\n",
    "\n",
    "for num, result in enumerate(results['change_models']):\n",
    "    print('Result: {}'.format(num))\n",
    "    print('Start Date: {}'.format(dt.date.fromordinal(result['start_day'])))\n",
    "    print('End Date: {}'.format(dt.date.fromordinal(result['end_day'])))\n",
    "    print('Break Date: {}'.format(dt.date.fromordinal(result['break_day'])))\n",
    "    print('QA: {}'.format(result['curve_qa']))\n",
    "    print('Change prob: {}'.format(result['change_probability']))\n",
    "    \n",
    "    days = np.arange(result['start_day'], result['end_day'] + 1)\n",
    "    \n",
    "    break_dates.append(result['break_day'])\n",
    "    start_dates.append(result['start_day'])\n",
    "    end_dates.append(result['end_day'])\n",
    "    \n",
    "    for b in bands:\n",
    "        band_info[b]['inter'] = result[b]['intercept']\n",
    "        band_info[b]['coefs'] = result[b]['coefficients']\n",
    "        band_info[b]['pred'] = predicts(days, result[b]['coefficients'], result[b]['intercept'])\n",
    "    \n",
    "        intercept = result[b]['intercept']\n",
    "        coef = result[b]['coefficients']\n",
    "        prediction_dates.append(days)\n",
    "        predicted_values.append(predicts(days, coef, intercept))\n",
    "    \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# dates_plt is the same as dates_masked\n",
    "# dates_plt = dates[mask]\n",
    "\n",
    "# ****X-Axis Ticks and Labels****\n",
    "# list of years\n",
    "y = [yi for yi in range(1981, 2018, 2)]\n",
    "\n",
    "# list of datetime objects with YYYY-MM-dd pattern\n",
    "t = [dt.datetime(yx, 7, 1) for yx in y]\n",
    "\n",
    "# list of ordinal time objects\n",
    "ord_time = [dt.datetime.toordinal(tx) for tx in t]\n",
    "\n",
    "# list of datetime formatted strings\n",
    "x_labels = [str(dt.datetime.fromordinal(int(L)))[:10] if L != \"0.0\" and L != \"\" else \"0\" for L in ord_time]\n",
    "\n",
    "total_mask = np.logical_and(mask, qa_in)\n",
    "\n",
    "for num, b in enumerate(bands):\n",
    "    fg = plt.figure(figsize=(16,9), dpi=300)\n",
    "    a1 = fg.add_subplot(2, 1, 1, xlim=(min(dates)-100, max(dates)+500), ylim=(min(data_in[num, total_mask]) - 500, \n",
    "                                                                                 max(data_in[num, total_mask]) + 500))\n",
    "    \n",
    "    \n",
    "    # data_plt = data[num, mask]\n",
    "    \n",
    "    # Observed values in PyCCD time range\n",
    "    a1.plot(dates_in[total_mask], data_in[num, total_mask], 'go', ms=7, mec='k', mew=0.5, label=\"Observations used by PyCCD\")\n",
    "    \n",
    "    # Observed values outside PyCCD time range\n",
    "    a1.plot(dates_out[qa_out], data_out[num][qa_out], 'ro', ms=5, mec='k', mew=0.5, label=\"Observations not used by PyCCD\")\n",
    "    \n",
    "    # Observed values masked out\n",
    "    a1.plot(dates_in[~mask], data_in[num, ~mask], color=\"0.65\", marker=\"o\", linewidth=0, ms=3, \n",
    "            label=\"Observations masked by PyCCD\")\n",
    "    \n",
    "    a1.set_title(f'{b}')\n",
    "    \n",
    "    # plot model break and start dates\n",
    "    match_dates = [b for b in break_dates for s in start_dates if b==s]\n",
    "    \n",
    "    for ind, e in enumerate(end_dates): \n",
    "        if ind == 0:\n",
    "            a1.axvline(e, color=\"black\", label=\"End dates\")\n",
    "        \n",
    "        else:\n",
    "            a1.axvline(e, color=\"black\")\n",
    "            \n",
    "    \n",
    "    for ind, b in enumerate(break_dates): \n",
    "        if ind == 0:\n",
    "            a1.axvline(b, color='r', label=\"Break dates\")\n",
    "        \n",
    "        else:\n",
    "            a1.axvline(b, color='r')\n",
    "        \n",
    "    for ind, s in enumerate(start_dates): \n",
    "        if ind ==0:\n",
    "            a1.axvline(s, color='b', label=\"Start dates\")\n",
    "        \n",
    "        else:\n",
    "            a1.axvline(s, color='b')\n",
    "            \n",
    "    for ind, m in enumerate(match_dates):\n",
    "        if ind == 0:\n",
    "            a1.axvline(m, color=\"magenta\", label=\"Break date = Start date\")\n",
    "        \n",
    "        else:\n",
    "            a1.axvline(m, color=\"magenta\")\n",
    "\n",
    "    # Predicted curves\n",
    "    for c in range(0 , len(results[\"change_models\"])):\n",
    "        if c == 0:\n",
    "            a1.plot(prediction_dates[c * len(bands) + num], predicted_values[c * len(bands) + num],\n",
    "                   \"orange\", linewidth=2, label=\"PyCCD model fit\")\n",
    "        \n",
    "        else:\n",
    "            a1.plot(prediction_dates[c * len(bands) + num], predicted_values[c * len(bands) + num],\n",
    "                   \"orange\", linewidth=2)\n",
    "        \n",
    "    # Add legend\n",
    "    a1.legend(mode=\"expand\", ncol=4, loc=\"lower center\")\n",
    "    \n",
    "    # Add x-ticks and x-tick_labels \n",
    "    a1.set_xticks(ord_time)\n",
    "\n",
    "    a1.set_xticklabels(x_labels, rotation=70, horizontalalignment=\"right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot indices\n",
    "NDVI = ndvi(NIR=data[3], R=data[2])\n",
    "MSAVI = msavi(NIR=data[3], R=data[2])\n",
    "EVI = evi(B=data[0], NIR=data[3], R=data[2])\n",
    "SAVI = savi(NIR=data[3], R=data[2])\n",
    "NDMI = ndmi(NIR=data[3], SWIR1=data[4])\n",
    "NBR = nbr(NIR=data[3], SWIR2=data[5])\n",
    "NBR2 = nbr2(SWIR1=data[4], SWIR2=data[5])\n",
    "\n",
    "NDVI_ = [ndvi(NIR=predicted_values[m * len(bands) + 3], R=predicted_values[m * len(bands) + 2]) \n",
    "         for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "MSAVI_ = [msavi(R=predicted_values[m * len(bands) + 2], NIR=predicted_values[m * len(bands) + 3]) \n",
    "          for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "EVI_ = [evi(B=predicted_values[m * len(bands)], NIR=predicted_values[m * len(bands) + 3], \n",
    "            R=predicted_values[m * len(bands) + 2]) for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "SAVI_ = [savi(NIR=predicted_values[m * len(bands) + 3], R=predicted_values[m * len(bands) + 2]) \n",
    "         for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "NDMI_ = [ndmi(NIR=predicted_values[m * len(bands) + 3], SWIR1=predicted_values[m * len(bands) + 4]) \n",
    "         for m in range(len(results[\"change_models\"]))]\n",
    "              \n",
    "NBR_ = [nbr(NIR=predicted_values[m * len(bands) + 3], SWIR2=predicted_values[m * len(bands) + 5]) \n",
    "        for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "NBR2_ = [nbr2(SWIR1=predicted_values[m * len(bands) + 4], SWIR2=predicted_values[m * len(bands) + 5]) \n",
    "         for m in range(len(results[\"change_models\"]))]\n",
    "\n",
    "\n",
    "indices = {\"ndvi\":(NDVI, NDVI_), \"msavi\":(MSAVI, MSAVI_), \"evi\":(EVI, EVI_), \"savi\":(SAVI, SAVI_), \n",
    "           \"ndmi\":(NDMI, NDMI_), \"nbr\":(NBR, NBR_), \"nbr2\":(NBR2, NBR2_)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ****X-Axis Ticks and Labels****\n",
    "\n",
    "# get year values for labeling plots\n",
    "year1 = str(dt.datetime.fromordinal(dates[0]))[:4]\n",
    "year2 = str(dt.datetime.fromordinal(dates[-1]))[:4]\n",
    "years = np.arange(int(year1), int(year2) + 2, 2)\n",
    "\n",
    "# list of datetime objects with YYYY-MM-dd pattern\n",
    "t = [dt.datetime(yx, 7, 1) for yx in years]\n",
    "\n",
    "# list of ordinal time objects\n",
    "ord_time = [dt.datetime.toordinal(tx) for tx in t]\n",
    "\n",
    "# list of datetime formatted strings\n",
    "x_labels = [str(dt.datetime.fromordinal(int(L)))[:10] if L != \"0.0\" and L != \"\" else \"0\" for L in ord_time]\n",
    "\n",
    "for num, i in enumerate(indices):\n",
    "    \n",
    "    index_in = indices[i][0][date_mask]\n",
    "    index_out = indices[i][0][~date_mask]\n",
    "    \n",
    "    fg = plt.figure(figsize=(16,9), dpi=300)\n",
    "    a1 = fg.add_subplot(2,1,1, ylim=(min(index_in[total_mask]) - 0.1, max(index_in[total_mask]) + 0.1))\n",
    "\n",
    "    a1.set_xticks(ord_time)\n",
    "    a1.set_xticklabels(x_labels, rotation=70, horizontalalignment=\"right\")\n",
    "\n",
    "    # plot observed calculated index values within PyCCD date range\n",
    "    a1.plot(dates_in[total_mask], index_in[total_mask], 'go', ms=7, mec='k', mew=0.5)\n",
    "    \n",
    "    # plot observed calculated index values masked out within PyCCD date range\n",
    "    a1.plot(dates_in[~total_mask][index_in[~total_mask] != 0], index_in[~total_mask][index_in[~total_mask] != 0], color=\"0.65\", ms=5, mec='k', mew=0.5, marker=\"o\", linewidth=0)\n",
    "    \n",
    "    # plot observed calculated index values outside of PyCCD date range\n",
    "    a1.plot(dates_out[qa_out], index_out[qa_out], \"ro\", ms=5, mec=\"k\", mew=0.5)\n",
    "    \n",
    "    # Predicted curves\n",
    "    for c in range(0 , len(results[\"change_models\"])):\n",
    "        a1.plot(prediction_dates[c * len(bands)], indices[i][1][c],\"orange\", linewidth=2)\n",
    "        \n",
    "        # plot model break and start dates\n",
    "    match_dates = [b for b in break_dates for s in start_dates if b==s]\n",
    "    for b in break_dates: a1.axvline(b, color='r')\n",
    "        \n",
    "    for s in start_dates: a1.axvline(s, color='b')\n",
    "\n",
    "    for m in match_dates: a1.axvline(m, color=\"magenta\")   \n",
    "    \n",
    "    a1.set_title(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = ('blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal')\n",
    "band_info = {b: {'coefs': [], 'inter': [], 'pred': []} for b in bands}\n",
    "\n",
    "mask = np.array(results['processing_mask'], dtype=bool)\n",
    "\n",
    "\"\"\"\n",
    "print('Start Date: {0}\\nEnd Date: {1}\\n'.format(dt.datetime.fromordinal(dates[0]),\n",
    "                                                dt.datetime.fromordinal(dates[-1])))\n",
    "\"\"\"\n",
    "\n",
    "predicted_values = []\n",
    "prediction_dates = []\n",
    "break_dates = []\n",
    "start_dates = []\n",
    "end_dates = []\n",
    "\n",
    "# get year values for labeling plots\n",
    "year1 = str(dt.datetime.fromordinal(dates[0]))[:4]\n",
    "year2 = str(dt.datetime.fromordinal(dates[-1]))[:4]\n",
    "years = np.arange(int(year1), int(year2), 2)\n",
    "\n",
    "for num, result in enumerate(results['change_models']):\n",
    "    print('Result: {}'.format(num))\n",
    "    print('Start Date: {}'.format(dt.date.fromordinal(result['start_day'])))\n",
    "    print('End Date: {}'.format(dt.date.fromordinal(result['end_day'])))\n",
    "    print('Break Date: {}'.format(dt.date.fromordinal(result['break_day'])))\n",
    "    print('QA: {}'.format(result['curve_qa']))\n",
    "    print('Change prob: {}'.format(result['change_probability']))\n",
    "    \n",
    "    days = np.arange(result['start_day'], result['end_day'] + 1)\n",
    "    \n",
    "    break_dates.append(result['break_day'])\n",
    "    start_dates.append(result['start_day'])\n",
    "    end_dates.append(result['end_day'])\n",
    "    \n",
    "    for b in bands:\n",
    "        band_info[b]['inter'] = result[b]['intercept']\n",
    "        band_info[b]['coefs'] = result[b]['coefficients']\n",
    "        band_info[b]['pred'] = predicts(days, result[b]['coefficients'], result[b]['intercept'])\n",
    "    \n",
    "        intercept = result[b]['intercept']\n",
    "        coef = result[b]['coefficients']\n",
    "        prediction_dates.append(days)\n",
    "        predicted_values.append(predicts(days, coef, intercept))\n",
    "    \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# dates_plt is the same as dates_masked\n",
    "# dates_plt = dates[mask]\n",
    "\n",
    "# ****X-Axis Ticks and Labels****\n",
    "# list of years\n",
    "y = [yi for yi in range(1981, 2018, 2)]\n",
    "\n",
    "# list of datetime objects with YYYY-MM-dd pattern\n",
    "t = [dt.datetime(yx, 7, 1) for yx in y]\n",
    "\n",
    "# list of ordinal time objects\n",
    "ord_time = [dt.datetime.toordinal(tx) for tx in t]\n",
    "\n",
    "# list of datetime formatted strings\n",
    "x_labels = [str(dt.datetime.fromordinal(int(L)))[:10] if L != \"0.0\" and L != \"\" else \"0\" for L in ord_time]\n",
    "\n",
    "total_mask = np.logical_and(mask, qa_in)\n",
    "\n",
    "for num, b in enumerate(bands):\n",
    "    fg = plt.figure(figsize=(16,9), dpi=300)\n",
    "    a1 = fg.add_subplot(2, 1, 1, xlim=(min(dates)-100, max(dates)+500), ylim=(min(data_in[num, total_mask]) - 500, \n",
    "                                                                                 max(data_in[num, total_mask]) + 500))\n",
    "    \n",
    "    \n",
    "    # data_plt = data[num, mask]\n",
    "    \n",
    "    # Observed values in PyCCD time range\n",
    "    a1.plot(dates_in[total_mask], data_in[num, total_mask], 'go', ms=7, mec='k', mew=0.5)\n",
    "    \n",
    "    # Observed values outside PyCCD time range\n",
    "    a1.plot(dates_out[qa_out], data_out[num][qa_out], 'ro', ms=5, mec='k', mew=0.5)\n",
    "    \n",
    "    # Observed values masked out\n",
    "    a1.plot(dates_in[~mask], data_in[num, ~mask], color=\"0.65\", marker=\"o\", linewidth=0, ms=3)\n",
    "    \n",
    "    a1.set_title(f'{b}')\n",
    "    \n",
    "    # plot model break and start dates\n",
    "    match_dates = [b for b in break_dates for s in start_dates if b==s]\n",
    "    for b in break_dates: a1.axvline(b, color='red')\n",
    "        \n",
    "    for s in start_dates: a1.axvline(s, color='b')\n",
    "\n",
    "    for e in end_dates: a1.axvline(e, color=\"black\")\n",
    "    \n",
    "    for m in match_dates: a1.axvline(m, color=\"magenta\")\n",
    "        \n",
    "    \n",
    "                \n",
    "    \n",
    "    # Predicted curves\n",
    "    for c in range(0 , len(results[\"change_models\"])):\n",
    "        a1.plot(prediction_dates[c * len(bands) + num], predicted_values[c * len(bands) + num],\n",
    "               \"orange\", linewidth=2)\n",
    "\n",
    "    # Add x-ticks and x-tick_labels \n",
    "    a1.set_xticks(ord_time)\n",
    "\n",
    "    a1.set_xticklabels(x_labels, rotation=70, horizontalalignment=\"right\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = ('ndvi', 'msavi', 'evi', 'savi', 'ndmi', 'nbr', 'nbr2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_lookup = {\"ndvi\": (NDVI, NDVI_), \"msavi\": (MSAVI, MSAVI_),\n",
    "                        \"evi\": (EVI, EVI_),\"savi\": (SAVI, SAVI_), \"ndmi\": (NDMI, NDMI_),\n",
    "                        \"nbr\": (NBR, NBR_),\"nbr2\": (NBR2, NBR2_)}\n",
    "\n",
    "band_lookup = {\"blue\" : data[0], \"green\" : data[1], \"red\" : [2], \"nir\" : [3], \"swir-1\" : [4], \"swir-2\" : [5], \"thermal\" : [6]}\n",
    "\n",
    "all_lookup = {**index_lookup, **band_lookup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
